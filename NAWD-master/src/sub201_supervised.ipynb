{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "498d8214",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a7c35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import scipy.io\n",
    "import mne\n",
    "import sklearn\n",
    "import os \n",
    "import random\n",
    "import time\n",
    "import scipy.linalg\n",
    "import pytorch_lightning as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "from itertools import chain, product\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.cross_decomposition import CCA\n",
    "from mne_features.feature_extraction import FeatureExtractor\n",
    "from torch.utils.data import random_split, DataLoader, Dataset\n",
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "from pytorch_lightning.core.module import LightningModule\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from scipy.stats import norm, wasserstein_distance\n",
    "from torchmetrics.classification import BinaryAccuracy\n",
    "\n",
    "%load_ext tensorboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50760fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11230cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c65991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assess whether GPU is availble\n",
    "if torch.cuda.is_available():\n",
    "    print(\"PyTorch is using the GPU.\")\n",
    "    print(\"Device name - \", torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "else: \n",
    "    print(\"PyTorch is not using the GPU.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068ca5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Utility functions frmo diffrent notebooks\n",
    "import import_ipynb\n",
    "from IEEE_data import extract_ieee_data, LazyProperty, data_4class\n",
    "from CHIST_ERA_data import *\n",
    "from Utils import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "77144ecd",
   "metadata": {},
   "source": [
    "### Datset and Model classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3851e687",
   "metadata": {},
   "outputs": [],
   "source": [
    "class convolution_AE(LightningModule):\n",
    "    def __init__(self, input_channels, days_labels_N, task_labels_N, learning_rate=1e-3, filters_n = [32, 16, 4], mode = 'supervised'):\n",
    "        super().__init__()\n",
    "        self.input_channels = input_channels\n",
    "        self.filters_n = filters_n\n",
    "        self.learning_rate = learning_rate\n",
    "        self.float()\n",
    "        self.l1_filters, self.l2_filters, self.l3_filters = self.filters_n\n",
    "        self.mode = mode\n",
    "        self.switcher = False\n",
    "        ### The model architecture ###\n",
    "        \n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "        nn.Conv1d(self.input_channels, self.l1_filters, kernel_size=25, stride=5, padding=1),\n",
    "#         nn.Dropout1d(p=0.2),\n",
    "#         nn.MaxPool1d(kernel_size=15, stride=3),\n",
    "        nn.LeakyReLU(),\n",
    "#         nn.AvgPool1d(kernel_size=2, stride=2),\n",
    "        nn.Conv1d(self.l1_filters, self.l2_filters, kernel_size=10, stride=2, padding=1),\n",
    "#         nn.Dropout1d(p=0.2),\n",
    "        nn.LeakyReLU(),\n",
    "#         nn.AvgPool1d(kernel_size=2, stride=2),\n",
    "        nn.Conv1d(self.l2_filters, self.l3_filters, kernel_size=5, stride=2, padding=1),\n",
    "#         nn.Dropout1d(p=0.2),\n",
    "        nn.LeakyReLU()\n",
    "        )\n",
    "                \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "        # IMPORTENT - on the IEEE dataset - the output padding needs to be 1 in the row below -on CHIST-ERA its 1\n",
    "        nn.ConvTranspose1d(self.l3_filters, self.l2_filters, kernel_size=5, stride=2, padding=1, output_padding=1),\n",
    "#         nn.Dropout1d(p=0.33),\n",
    "        nn.LeakyReLU(),\n",
    "#         nn.Upsample(scale_factor=2, mode='linear'),\n",
    "        nn.ConvTranspose1d(self.l2_filters, self.l1_filters, kernel_size=10, stride=2, padding=1, output_padding=0),\n",
    "#         nn.Dropout1d(p=0.33),\n",
    "        nn.LeakyReLU(),\n",
    "#         nn.Upsample(scale_factor=2, mode='linear'),\n",
    "        nn.ConvTranspose1d(self.l1_filters, self.input_channels, kernel_size=25, stride=5, padding=1, output_padding=0),\n",
    "        )\n",
    "        \n",
    "        # Residuals Encoder\n",
    "        self.res_encoder = nn.Sequential(\n",
    "        nn.Conv1d(self.input_channels, self.l1_filters, kernel_size=25, stride=5, padding=1),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Conv1d(self.l1_filters, self.l2_filters, kernel_size=10, stride=2, padding=1),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Conv1d(self.l2_filters, self.l3_filters, kernel_size=5, stride=2, padding=1),\n",
    "        nn.LeakyReLU()\n",
    "        )\n",
    "                \n",
    "        # Classifier Days\n",
    "        self.classiffier_days = nn.Sequential(\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(1120, days_labels_N),\n",
    "        nn.Dropout(0.5),\n",
    "        )\n",
    "        \n",
    "        # Classifier Task\n",
    "        self.classiffier_task = nn.Sequential(\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(1120, task_labels_N),\n",
    "        nn.Dropout(0.5),\n",
    "\n",
    "        )\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Forward through the layeres\n",
    "        # Encoder\n",
    "        x = self.encoder(x)\n",
    "\n",
    "        # Decoder\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "    \n",
    "    def encode(self, x):\n",
    "        # Forward through the layeres\n",
    "        # Encoder\n",
    "        x = self.encoder(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def on_train_epoch_end(self):\n",
    "        if self.current_epoch > 450:\n",
    "            self.unfreeze_decoder()\n",
    "            self.unfreeze_encoder()\n",
    "            self.mode = 'all'\n",
    "    \n",
    "        elif self.current_epoch % 20 == 0:\n",
    "            self.switcher = not self.switcher\n",
    "            if self.switcher == True:\n",
    "                self.freeze_decoder()\n",
    "                self.unfreeze_encoder()\n",
    "                self.mode = 'task'\n",
    "            elif self.switcher == False:\n",
    "                self.freeze_encoder()\n",
    "                self.unfreeze_decoder()\n",
    "                self.mode = 'reconstruction'\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Extract batch\n",
    "        x, y, days_y = batch\n",
    "        # Define loss functions\n",
    "        loss_fn_days = nn.CrossEntropyLoss()\n",
    "        loss_fn_rec = nn.MSELoss()\n",
    "        loss_fn_task = nn.CrossEntropyLoss()\n",
    "            \n",
    "        # Encode\n",
    "        encoded = self.encode(x)\n",
    "        \n",
    "        # Get predictions for task\n",
    "        preds_task = self.classiffier_task(encoded)\n",
    "        task_loss = loss_fn_task(preds_task, y)\n",
    "\n",
    "        # Compute task classification accuracy\n",
    "        task_acc = sklearn.metrics.accuracy_score(np.argmax(F.softmax(preds_task, dim=-1).detach().cpu().numpy(), axis=1),\n",
    "                                             np.argmax(y.detach().cpu().numpy(), axis=1))\n",
    "\n",
    "        # Log scalars\n",
    "        self.log('task_loss', task_loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        self.log('task_accuracy', task_acc, prog_bar=True, on_step=False, on_epoch=True)\n",
    "\n",
    "        # Decode\n",
    "        reconstructed = self.decoder(encoded)\n",
    "\n",
    "        # Compute residuals\n",
    "        residuals = torch.sub(x, reconstructed)\n",
    "\n",
    "        # Encode residuals\n",
    "        residuals_compact = self.res_encoder(residuals)\n",
    "\n",
    "        # Get predictions per day\n",
    "        preds_days = self.classiffier_days(residuals_compact)\n",
    "\n",
    "        # Compute all losses\n",
    "        days_loss = loss_fn_days(preds_days, days_y)\n",
    "        reconstruction_loss = loss_fn_rec(reconstructed, x)\n",
    "\n",
    "        # Compute days classification accuracy\n",
    "        days_acc = sklearn.metrics.accuracy_score(np.argmax(F.softmax(preds_days, dim=-1).detach().cpu().numpy(), axis=1),\n",
    "                                             np.argmax(days_y.detach().cpu().numpy(), axis=1))\n",
    "\n",
    "        # Log results\n",
    "        self.log('days_loss', days_loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        self.log('reconstruction_loss', reconstruction_loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        self.log('days_accuracy', days_acc, prog_bar=True, on_step=False, on_epoch=True)\n",
    "\n",
    "        if self.mode == 'task':\n",
    "            return task_loss + days_loss\n",
    "        elif self.mode == 'reconstruction':\n",
    "            return reconstruction_loss \n",
    "        elif self.mode == 'all':\n",
    "            return reconstruction_loss + days_loss + task_loss\n",
    "   \n",
    "    def get_lr(optimizer):\n",
    "        for param_group in optimizer.param_groups:\n",
    "            return param_group['lr']\n",
    "    \n",
    "    \n",
    "    def freeze_encoder(self):\n",
    "        for name, param in self.encoder.named_parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "    def unfreeze_encoder(self):\n",
    "        for name, param in self.encoder.named_parameters():\n",
    "            param.requires_grad = True\n",
    "            \n",
    "    def freeze_decoder(self):\n",
    "        for name, param in self.decoder.named_parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "    def unfreeze_decoder(self):\n",
    "        for name, param in self.decoder.named_parameters():\n",
    "            param.requires_grad = True\n",
    "            \n",
    "            \n",
    "    def change_mode(self, mode):\n",
    "        self.mode = mode\n",
    "        \n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        # Optimizer\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6bfa2af9",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "channels names:\n",
    "['FC3', 'C1', 'C3', 'C5', 'CP3', 'O1', 'FC4', 'C2', 'C4', 'C6', 'CP4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0205cc58",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "subID = '201' # As str 201, 205, 206\n",
    "eyesFlag = 'CC' # str        CC --> closed,   OO --> open\n",
    "dataDir = '../data'\n",
    "results_data_dir = '../results_dir'\n",
    "logdir = '../tb_logs'\n",
    "\n",
    "# To get all The days in subject 201:\n",
    "dayNumber = get_all_days(dataDir, subID, eyesFlag) # Array of the desired days number\n",
    "dayNumber.sort()\n",
    "# For subject 205 & 206 its better to insert range\n",
    "# dayNumber = range(1,9)\n",
    "\n",
    "# Subject 201 has only 1 block\n",
    "block = [1]\n",
    "trialLen = 6 # In seconds\n",
    "filterLim = [1, 40] # In Hz\n",
    "elec_idxs = range(11) # 0-10 according to channel names\n",
    "train_days=[0,40]\n",
    "\n",
    "\n",
    "\n",
    "ae_learning_rt = 3e-4\n",
    "n_epochs = 250\n",
    "batch_sz = 16\n",
    "# If you want to use comparison rate - set layers_sz = False\n",
    "convolution_filters = [8,16,32] # Length = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c76538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert relative path to absolute path\n",
    "dataDir = os.path.abspath(dataDir)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1c42c491",
   "metadata": {},
   "source": [
    "### Load the files - CHIST ERA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac98a176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all relevant days files into list\n",
    "dataList = getRecording(dataDir, subID, eyesFlag, dayNumber, block)\n",
    "\n",
    "# Extract and segment all the data\n",
    "dictList = []\n",
    "for dayData in dataList:\n",
    "    # Extract each day data\n",
    "    interData = extractData(dayData)\n",
    "    \n",
    "    # This condition is to remove some corrupted files in subject 201\n",
    "    if interData['EEG'].dtype != np.dtype('float64'):\n",
    "        continue\n",
    "        \n",
    "    # Filter the data\n",
    "    interData['EEG'] = eegFilters(interData['EEG'], interData['fs'], filterLim)\n",
    "    interData['EEG'] = interData['EEG'][elec_idxs, :]\n",
    "\n",
    "    # Segment the data\n",
    "    dictList.append(segmentEEG(interData, trialLen, printFlag=0))\n",
    "\n",
    "# Stack block of same day\n",
    "dictListStacked = stackBlocks(dictList, len(block))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "66d26a62",
   "metadata": {},
   "source": [
    "# Training loop function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55016a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(train_days, dictListStacked, ae_learning_rt, convolution_filters, batch_sz, epoch_n):\n",
    "    \n",
    "    device = torch.device(\"cuda\")\n",
    "    # Logger\n",
    "    logger = TensorBoardLogger('../tb_logs', name='EEG_Logger')\n",
    "    # Shuffle the days\n",
    "    random.shuffle(dictListStacked)\n",
    "    # Train Dataset\n",
    "    signal_data = EEGDataSet_signal_by_day(dictListStacked, train_days)\n",
    "    signal_data_loader = DataLoader(dataset=signal_data, batch_size=batch_sz, shuffle=True, num_workers=0)\n",
    "    x, y, days_y = signal_data.getAllItems()\n",
    "    y = np.argmax(y, -1)\n",
    "    days_labels_N = signal_data.days_labels_N\n",
    "    task_labels_N = signal_data.task_labels_N\n",
    "\n",
    "    # Train model on training day\n",
    "    metrics = ['classification_loss', 'reconstruction_loss']\n",
    "    day_zero_AE = convolution_AE(signal_data.n_channels, days_labels_N, task_labels_N, ae_learning_rt, filters_n=convolution_filters, mode='supervised')\n",
    "    day_zero_AE.to(device)\n",
    "\n",
    "    trainer_2 = pl.Trainer(max_epochs=epoch_n, logger=logger, accelerator='gpu', devices=-1)\n",
    "    trainer_2.fit(day_zero_AE, train_dataloaders=signal_data_loader)\n",
    "    \n",
    "    # CV On the training set (with and without ae)\n",
    "    score_ae, day_zero_AE_clf = csp_score(np.float64(day_zero_AE(x).detach().numpy()), y, cv_N=5, classifier=False)\n",
    "    score_bench, day_zero_bench_clf = csp_score(np.float64(x.detach().numpy()), y, cv_N=5, classifier=False)\n",
    "    \n",
    "    # Loop :)\n",
    " \n",
    "    # Append ws for normal method\n",
    "    bench_same_day_score = score_bench\n",
    "    # Append ws with ae\n",
    "    AE_same_day_score = score_ae\n",
    "\n",
    "    test_days = [train_days[1], len(dictListStacked)]\n",
    "\n",
    "    # Create test Datasets\n",
    "    signal_test_data = EEGDataSet_signal(dictListStacked, test_days)\n",
    "\n",
    "    # get data\n",
    "    signal_test, y_test = signal_test_data.getAllItems()\n",
    "    # reconstruct EEG using day 0 AE\n",
    "    rec_signal_zero = day_zero_AE(signal_test).detach().numpy()\n",
    "\n",
    "\n",
    "    # Use models\n",
    "    # within session cv on the test set (mean on test set)\n",
    "    ws_test, _ = csp_score(np.float64(signal_test.detach().numpy()), y_test, cv_N=5, classifier = False)\n",
    "    # Using day 0 classifier for test set inference (mean on test set)\n",
    "    bs_test = csp_score(np.float64(signal_test.detach().numpy()), y_test, cv_N=5, classifier=day_zero_bench_clf)\n",
    "    # Using day 0 classifier + AE for test set inference (mean on test set)\n",
    "    bs_ae_test = csp_score(rec_signal_zero, y_test, cv_N=5, classifier=day_zero_AE_clf)\n",
    "    \n",
    "    return bench_same_day_score, AE_same_day_score, ws_test, bs_test, bs_ae_test, day_zero_AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e5f77c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "per_day_score = []\n",
    "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "for d in dictListStacked:\n",
    "    temp_score = []\n",
    "    for i in range(100):\n",
    "        score_bench, _ = csp_score(np.float64(d['segmentedEEG']), d['labels'], cv_N=5, classifier=False)\n",
    "        temp_score.append(score_bench)\n",
    "    per_day_score.append(np.mean(temp_score))\n",
    "\n",
    "with open(results_data_dir+'/per_day_score_' + timestr + '.pickle', 'wb') as f:\n",
    "    pickle.dump(per_day_score, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0798907e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sub 206 - 200 epochs\n",
    "train_days=[0,40]\n",
    "\n",
    "bench_same_day_score, bench_diff_day_score, AE_diff_day_score, model = \\\n",
    "training_loop(train_days, dictListStacked, dictListStacked[0]['fs'], ae_learning_rt, convolution_filters, batch_sz, n_epochs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "222931dd",
   "metadata": {},
   "source": [
    "# Training for several days loop function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479d524c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_over_number_of_days(start_day, epoch_n, dictListStacked, fs, ae_learning_rt, \\\n",
    "                              convolution_filters, batch_sz, max_delta=30):\n",
    "    bench_diff_day_score_mean = []\n",
    "    AE_diff_day_score_mean = []\n",
    "    bench_same_day_score_mean = []\n",
    "    \n",
    "    ws_train_score = []\n",
    "    ae_train_score = []\n",
    "    \n",
    "    for delta in range(1, len(dictListStacked) - start_day):\n",
    "        \n",
    "        if delta > max_delta:\n",
    "            break\n",
    "        \n",
    "        train_days=[start_day, start_day + delta]\n",
    "\n",
    "        device = torch.device(\"cuda\")\n",
    "        # Logger\n",
    "        logger = TensorBoardLogger('../tb_logs', name='EEG_Logger')\n",
    "        # Shuffle the days\n",
    "        random.shuffle(dictListStacked)\n",
    "        # Train Dataset\n",
    "        signal_data = EEGDataSet_signal_by_day(dictListStacked, train_days)\n",
    "        signal_data_loader = DataLoader(dataset=signal_data, batch_size=batch_sz, shuffle=True, num_workers=0)\n",
    "        x, y, days_y = signal_data.getAllItems()\n",
    "        y = np.argmax(y, -1)\n",
    "        days_labels_N = signal_data.days_labels_N\n",
    "        task_labels_N = signal_data.task_labels_N\n",
    "\n",
    "        \n",
    "        # Train model on training day\n",
    "        metrics = ['classification_loss', 'reconstruction_loss']\n",
    "        day_zero_AE = convolution_AE(signal_data.n_channels, days_labels_N, task_labels_N, ae_learning_rt,\\\n",
    "                                     filters_n=convolution_filters, mode='supervised')\n",
    "        day_zero_AE.to(device)\n",
    "\n",
    "        trainer_2 = pl.Trainer(max_epochs=epoch_n, logger=logger, accelerator='gpu', devices=-1)\n",
    "        trainer_2.fit(day_zero_AE, train_dataloaders=signal_data_loader)\n",
    "\n",
    "        score_ae, day_zero_AE_clf = csp_score(np.float64(day_zero_AE(x).detach().numpy()), y, cv_N=5, classifier=False)\n",
    "        score_bench, day_zero_bench_clf = csp_score(np.float64(x.detach().numpy()), y, cv_N=5, classifier=False)\n",
    "        \n",
    "        ws_train_score.append(score_bench)\n",
    "        ae_train_score.append(score_ae)      \n",
    "\n",
    "        # Create test Datasets\n",
    "        signal_test_data = EEGDataSet_signal(dictListStacked, [train_days[1], len(dictListStacked)])\n",
    "        signal_test_data_loader = DataLoader(dataset=signal_test_data, batch_size=8, shuffle=True, num_workers=0)\n",
    "\n",
    "        # get data\n",
    "        signal_test, y_test = signal_test_data.getAllItems()\n",
    "        # reconstruct EEG using day 0 AE\n",
    "        rec_signal_zero = day_zero_AE(signal_test).detach().numpy()\n",
    "\n",
    "\n",
    "        # Use models\n",
    "        bench_diff_day = csp_score(np.float64(signal_test.detach().numpy()), y_test, cv_N = 5, classifier = day_zero_bench_clf)\n",
    "        AE_diff_day = csp_score(np.float64(rec_signal_zero), y_test, cv_N = 5, classifier = day_zero_AE_clf)\n",
    "\n",
    "\n",
    "        # Rest of the days cross validation score\n",
    "        score_bench, _= csp_score(np.float64(signal_test.detach().numpy()), y_test, cv_N = 5, classifier = False)\n",
    "        \n",
    "        # Append means\n",
    "        bench_diff_day_score_mean.append(bench_diff_day)\n",
    "        AE_diff_day_score_mean.append(AE_diff_day)\n",
    "        bench_same_day_score_mean.append(score_bench)\n",
    "       \n",
    "    # Convert results to numpy\n",
    "    bench_same_day_score_mean = np.asarray(bench_same_day_score_mean)\n",
    "    bench_diff_day_score_mean = np.asarray(bench_diff_day_score_mean)\n",
    "    AE_diff_day_score_mean = np.asarray(AE_diff_day_score_mean)\n",
    "\n",
    "    \n",
    "    # Return results\n",
    "    return bench_same_day_score_mean, bench_diff_day_score_mean, AE_diff_day_score_mean,\\\n",
    "            ae_train_score, ws_train_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f606f49f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Start from which day to plot?\n",
    "plot_from = 1\n",
    "\n",
    "# Plot\n",
    "plt.plot(range(plot_from, plot_from + len(AE_diff_day_score[plot_from:])), AE_diff_day_score[plot_from:], label='AE diff day', color='g')\n",
    "plt.plot(range(plot_from, plot_from + len(AE_diff_day_score[plot_from:])), bench_diff_day_score[plot_from:], label='bench diff day', color='r')\n",
    "plt.plot(range(plot_from, plot_from + len(AE_diff_day_score[plot_from:])), bench_same_day_score[plot_from:], label='bench same day', color='b')\n",
    "\n",
    "plt.axhline(y=np.mean(AE_diff_day_score[plot_from:]), color='g', linestyle='--')\n",
    "plt.axhline(y=np.mean(bench_diff_day_score[plot_from:]), color='r', linestyle='--')\n",
    "plt.axhline(y=np.mean(bench_same_day_score[plot_from:]), color='b', linestyle='--')\n",
    "\n",
    "plt.title('Accuracy Over Days - Using Day 0 Classifier')\n",
    "plt.xlabel('Day #')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c686000",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_over_number_of_days_train_cv(start_day, epoch_n, dictListStacked, fs, ae_learning_rt, \\\n",
    "                              convolution_filters, batch_sz, max_delta=30):\n",
    "    ws_train_list = []\n",
    "    \n",
    "    for delta in range(1, len(dictListStacked) - start_day):\n",
    "        \n",
    "        if delta > max_delta:\n",
    "            break\n",
    "        \n",
    "        train_days=[start_day, start_day + delta]\n",
    "        # Train Dataset\n",
    "        random.shuffle(dictListStacked)\n",
    "        signal_data = EEGDataSet_signal_by_day(dictListStacked, train_days)\n",
    "        signal_data_loader = DataLoader(dataset=signal_data, batch_size=batch_sz, shuffle=True, num_workers=0)\n",
    "        x, y, days_y = signal_data.getAllItems()\n",
    "        y = np.argmax(y, -1)\n",
    "\n",
    "\n",
    "        score_bench, day_zero_bench_clf = csp_score(np.float64(x.detach().numpy()), y, cv_N=5, classifier=False)\n",
    "        \n",
    "        \n",
    "        ws_train_list.append(score_bench)\n",
    "\n",
    "    # Return results\n",
    "    return ws_train_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770d74d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_data = EEGDataSet_signal(dictListStacked, [3,4])\n",
    "signal_data_loader = DataLoader(dataset=signal_data, batch_size=batch_sz, shuffle=True, num_workers=0)\n",
    "plotSignal(0, day_zero_AE, signal_data_loader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "86e20f4f",
   "metadata": {},
   "source": [
    "## Several days realizations (long time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247eca1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.abspath(logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d84ce96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ws_list = []\n",
    "bs_list = []\n",
    "ae_list = []\n",
    "ws_train_list = []\n",
    "ae_train_list = []\n",
    "\n",
    "# Get current time for file name\n",
    "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "for i in range(50):\n",
    "    print('Iter: ', i)\n",
    "    bench_same_day_score_mean, bench_diff_day_score_mean, AE_diff_day_score_mean,\\\n",
    "    ws_train_score, ae_train_score = \\\n",
    "    score_over_number_of_days(0, 500, dictListStacked, 128, ae_learning_rt,\\\n",
    "                              convolution_filters, batch_sz, max_delta=130)\n",
    "    ws_list.append(bench_same_day_score_mean)\n",
    "    bs_list.append(bench_diff_day_score_mean)\n",
    "    ae_list.append(AE_diff_day_score_mean)\n",
    "    ws_train_list.append(ws_train_score)\n",
    "    ae_train_list.append(ae_train_score)\n",
    "    \n",
    "    # Each iteration save locally the results\n",
    "    save_obj = (ws_list, bs_list, ae_list, ws_train_list, ae_train_list)\n",
    "    # Save the lists\n",
    "    with open(results_data_dir+'/201_results_500_epoch' + timestr + '.pickle', 'wb') as f:\n",
    "        pickle.dump(save_obj, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afe1a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "ws_list_all = []\n",
    "bs_list_all = []\n",
    "ae_list_all = []\n",
    "ws_train_all = []\n",
    "ae_train_all = []\n",
    "# load all pickle files\n",
    "for file in os.listdir(results_data_dir):\n",
    "    if file.endswith(\".pickle\"):\n",
    "        if 'new' in file and 'uns' not in file:\n",
    "#         if 'uns' in file:\n",
    "            with open(results_data_dir + '/' + file, \"rb\") as f:\n",
    "                load_obj = pickle.load(f)\n",
    "                ws_list_all.append(load_obj[0])\n",
    "                bs_list_all.append(load_obj[1])\n",
    "                ae_list_all.append(load_obj[2])\n",
    "                ws_train_all.append(load_obj[3])\n",
    "                ae_train_all.append(load_obj[4])\n",
    "        if 'per_day_score' in file:\n",
    "            with open(results_data_dir + '/' + file, \"rb\") as f:\n",
    "                per_day_score = pickle.load(f)\n",
    "\n",
    "\n",
    "#  Flatten the lists\n",
    "ws_means = np.mean(np.asarray([j for i in ws_list_all for j in i]), axis=0)\n",
    "bs_means = np.mean(np.asarray([j for i in bs_list_all for j in i]), axis=0)\n",
    "ae_means = np.mean(np.asarray([j for i in ae_list_all for j in i]), axis=0)\n",
    "ws_train_means = np.mean(np.asarray([j for i in ws_train_all for j in i]), axis=0)\n",
    "ae_train_means = np.mean(np.asarray([j for i in ae_train_all for j in i]), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2b77a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.asarray([j for i in ws_train_all for j in i]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a1ba4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ax = range(1,1+len(ws_means))\n",
    "# Plots Results\n",
    "plt.plot(x_ax, ws_train_means, label='WS train', color='b')\n",
    "plt.plot(x_ax, ae_train_means, label='AE train', color='g')\n",
    "\n",
    "plt.axhline(0.5, label='Chance level', color='k', linestyle='--')\n",
    "\n",
    "# Figure stuff\n",
    "plt.title('Mean Accuracy Score Over Days As Function Of Number Of Training Days')\n",
    "plt.xlabel('Number of Training Days')\n",
    "plt.ylabel('Mean Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d56b17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c66c3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ax = range(1,1+len(ws_means))\n",
    "# Plots Results\n",
    "plt.plot(x_ax, ae_means, label='AE score', color='g')\n",
    "plt.plot(x_ax, bs_means, label='BS score', color='r')\n",
    "# plt.plot(x_ax, ws_means, label='WS on test', color='teal')\n",
    "plt.plot(x_ax, ws_train_means, label='WS on train', color='b')\n",
    "plt.axhline(np.mean(per_day_score), label='Mean per day score', color='orange', linestyle='--')\n",
    "plt.axhline(0.5, label='Chance level', color='k', linestyle='--')\n",
    "# Figure stuff\n",
    "plt.title('Mean Accuracy Score Over Days As Function Of Number Of Training Days')\n",
    "plt.xlabel('Number of Training Days')\n",
    "plt.ylabel('Mean Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005b42b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_means_uns = ae_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01b0e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_ax[:100], ae_means[:100], label='AE score', color='g')\n",
    "plt.plot(x_ax[:100], ae_means_uns, label='AE score', color='b')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1df783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# import time\n",
    "# timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# save_obj = (ws_list, bs_list, ae_list)\n",
    "# # Save the lists\n",
    "# with open('201_results' + timestr + '.pickle', 'wb') as f:\n",
    "#     pickle.dump(save_obj, f)\n",
    "\n",
    "# # Load the lists\n",
    "# with open('201_results' + timestr + '.pickle', 'rb') as f:\n",
    "#     loaded_obj = pickle.load(f)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0707b6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ws_means = np.mean(np.asarray(ws_list), axis=0)\n",
    "bs_means = np.mean(np.asarray(bs_list), axis=0)\n",
    "ae_means = np.mean(np.asarray(ae_list), axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfd535e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trials_n_per_day = [len(day_dict['labels']) for day_dict in dictListStacked]\n",
    "\n",
    "print('Total number of days- ', len(dictListStacked), '\\nMean Trials count per day- ', np.mean(trials_n_per_day), '\\nTrials count std- ', np.std(trials_n_per_day))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "35d426c32d3c3d4800097806a9597474a84165f20bd11beece54e5b9a2bb14ee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
